# ğŸ”Œ Plug & Play â€” Agentic RAG Memory Engine

**Persistent, session-aware memory for any local LLM project.**  
Drop `memory_engine.py` into your project and call one function.

> Validated at **512,000 token depth** â€” 5/5 perfect recall using `phi4-mini:3.8b` on a single RTX 5070 12GB.

---

## âš¡ 5-Minute Setup

### 1. Install dependencies

```bash
pip install chromadb ollama pynvml
```

### 2. Pull the models

```bash
ollama pull phi4-mini:3.8b       # LLM â€” 2.5GB, 5/5 at 512k depth
ollama pull nomic-embed-text     # Embeddings â€” 274MB, needed for search
```

### 3. Copy the file into your project

```
your-project/
â”œâ”€â”€ memory_engine.py   â† copy this in
â””â”€â”€ your_app.py
```

### 4. Use it

```python
from memory_engine import chat_logic

answer, _ = chat_logic("What did we decide about the database schema?")
print(answer)
```

That's it. Memory is stored automatically and persists across restarts.

---

## âš™ï¸ Configuration

Open `memory_engine.py` and edit **SECTION 2** at the top:

```python
LLM_MODEL   = "phi4-mini:3.8b"      # swap to llama3.2:3b for lighter footprint
EMBED_MODEL = "nomic-embed-text"     # don't change this
DB_PATH     = "./memory_db"          # where your memory is stored on disk
OLLAMA_URL  = "http://localhost:11434"  # change if Ollama is on another machine
NUM_CTX     = 4096                   # 4096=safe, 8192=better quality
IDLE_TIMEOUT_SECONDS = 300           # new session block after 5min idle
```

All other code can be left untouched.

---

## ğŸ“‹ Model Comparison

| Model | Score | Ingestion Time | VRAM |
|-------|:-----:|:--------------:|:----:|
| `phi4-mini:3.8b` â­ | **5/5** | 147s | ~6.4GB |
| `llama3.2:3b` | 4/5 | 134s | ~2.5GB |
| `llama3.1:8b` | untested | â€” | ~5GB |

> **Live VRAM log** (phi4-mini + nomic-embed-text running concurrently):  
> `total: 12.8GB Â· free: 6.45GB Â· used: ~6.37GB`

---

## ğŸ§© Integration Examples

See [`example_chat.py`](example_chat.py) for ready-to-run code:

| Example | Use case |
|---------|----------|
| `example_1_simple()` | Store a fact, recall it later |
| `example_2_interactive_loop()` | Standalone chatbot |
| `example_3_agent_wrapper()` | Drop into any AI agent |
| `example_4_api_handler()` | FastAPI / Flask REST endpoint |
| `example_5_batch_ingest()` | Pre-load documents before a chat |

Run interactively:
```bash
python example_chat.py
```

---

## ğŸ” How It Works (brief)

```
Your message
  â†“  1. LLM rewrites your query into dense search keywords
  â†“  2. ChromaDB finds top 10 closest chunks (vector search)
  â†“  3. LLM picks the best match (agentic routing)
  â†“  4. Entire conversation block reassembled around the match
  â†“  5. Key entities (names, numbers, dates) prepended as cheat sheet
  â†“  6. Long context split into pages, each read independently
  â†“  7. Final answer generated (4,096 token context window)
  â†“  8. Conversation saved to ChromaDB in background thread
```

**Key insight:** The LLM never sees more than 4,096 tokens at once. The RAG pipeline surfaces only the right ~1,500 characters from a database up to 512,000 tokens deep.

---

## ğŸ“ Files

| File | Description |
|------|-------------|
| `memory_engine.py` | The engine â€” copy this into your project |
| `example_chat.py` | Integration examples for 5 common use cases |
| `README.md` | This file |

---

## ğŸ› Troubleshooting

**"connection refused" error**  
â†’ Make sure Ollama is running: `ollama serve`

**Model not found**  
â†’ Pull it first: `ollama pull phi4-mini:3.8b`

**Out of VRAM**  
â†’ Lower `NUM_CTX` to 2048, or switch to `llama3.2:3b` (lighter)

**Wrong answers**  
â†’ Try being more specific: `"Please remember: budget is exactly $120,000"` instead of `"the budget is 120k"`

**Memory grows too large**  
â†’ Delete the `./memory_db/` folder to start fresh. Or change `DB_PATH` to a new folder.

---

## ğŸ“„ From the research

This engine was the best performer across 6 experiments at 512k token depth.  
Full benchmark results and technical paper: [github.com/mhndayesh/infinite-context-rag](https://github.com/mhndayesh/infinite-context-rag)
