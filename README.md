# Agentic RAG Memory Architecture

> **Persistent, session-aware memory for local LLMs â€” validated at 100% (25/25) accuracy on 512,000 token depth.**  
> No fine-tuning. No cloud API. Zero cost per query. Complete data privacy.

---

## ğŸš€ NEW: Phase 16 Breakthrough (Feb 24, 2026)
The architecture has been upgraded to a **High-Precision Parallel Engine**. 

**Latest Stats:**
- **Accuracy:** 100% (25/25) across all depths.
- **Speed:** ~7.5s total pipeline (3x speedup via 16 parallel extraction slots).
- **Hardened:** Zero safety refusals via the novel **Direct-Return Bypass**.

### ğŸ“‚ Repository Structure

| Folder | Description |
| :--- | :--- |
| [**ğŸš€ apps/**](apps/) | **Start Here.** User-friendly chat interface and data ingestor. |
| [**ğŸ† core/**](core/) | The production-ready 100% accuracy RAG engine. |
| [**ğŸ“– docs/**](docs/) | Technical papers and the 16-phase architectural journey. |
| [**ğŸ“¦ archive/**](archive/) | All 15+ experimental phases, research logs, and legacy code. |

---

## ğŸ›  Quick Start

1. **Start LM Studio Server** (Set slots to 16, context to 8192).
2. **Ingest Data:** Drop `.txt` files in `apps/source_docs/` and run `python apps/INGEST_DATA.py`.
3. **Chat:** Run `python apps/QUICK_START_CHAT.py`.

---

## ğŸ† THE ULTIMATE VICTORY: 100% Recall at 512k Context

The system has overcome the "Semantic Noise" ceiling by implementing **Hybrid Stage 1 Retrieval (RRF)**. It now retrieves "hidden needles" with perfect precision even when they are buried at the very beginning of a 2,000,000-character document.

- **Stage 1 Recall:** 100% (Vector + BM25 Hybrid)
- **Stage 2 Routing:** 100% (BM25 Reranker)
- **Total Pipeline:** ~6-8s (Local `deepseek-r1:8b`)

---

## ğŸ“– Key Documentation
- [**FULL ARCHITECTURAL JOURNEY**](docs/FULL_ARCHITECTURAL_JOURNEY_INFINITE_CONTEXT_RAG.md) - The 16-phase chronicle from failure to 100% success.
- [**TECHNICAL DEEP DIVE**](docs/TECHNICAL_DEEP_DIVE.md) - Deep math/logic behind RRF, Parallel Mapping, and Bypass.
- [**VALUE PROPOSITION**](archive/perfect%20recal%20512k/VALUE_PROPOSITION.md) - Why local infinite-context RAG is the future.

---

## ğŸ“Š Historical Milestone: `phi4-mini:3.8b` + Baseline RAG â€” **5/5 Score**
*(Previous breakthrough on 32k-128k context ranges)*

> ğŸ“ Code: [`archive/experiment_5_phi4mini_baseline/`](archive/experiment_5_phi4mini_baseline/)  
> ğŸ“„ Results: [`archive/research_papers_and_reports/session_512k_accuracy_report.md`](archive/research_papers_and_reports/session_512k_accuracy_report.md)

---

## ğŸ“Š Full Benchmark â€” All 4 Methods Ã— Both Models

| Method | `llama3.2:3b` Score | `llama3.2:3b` Time | `phi4-mini:3.8b` Score | `phi4-mini:3.8b` Time |
|--------|:-------------------:|:------------------:|:----------------------:|:---------------------:|
| **Baseline RAG** â­ | 4/5 | 134s | **5/5** âœ… | **147s** |
| **Forced CoT** `<think>` | 1/5 | 190s | **5/5** âœ… | 235s |
| **Agentic Ctrl-F** | 2/5 | 187s | 3/5 | 280s |

> Hardware: Intel i7-14700F Â· 64GB RAM Â· NVIDIA RTX 5070 12GB VRAM Â· Ollama local inference  

---

## Requirements
- Python 3.10+
- `chromadb`
- `ollama` (for embeddings)
- `lm-studio` (for high-speed parallel inference)
- `pynvml` (optional â€” VRAM monitoring)
